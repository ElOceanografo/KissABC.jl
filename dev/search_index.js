var documenterSearchIndex = {"docs":
[{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"EditURL = \"https://github.com/JuliaApproxInference/KissABC.jl/blob/master/docs/literate/example_1.jl\"","category":"page"},{"location":"example_1/#A-gaussian-mixture-model","page":"Example: Gaussian Mixture","title":"A gaussian mixture model","text":"","category":"section"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"First of all we define our model,","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"using KissABC\nusing Distributions\n\nfunction model(P, N)\n    μ_1, μ_2, σ_1, σ_2, prob = P\n    d1 = randn(N) .* σ_1 .+ μ_1\n    d2 = randn(N) .* σ_2 .+ μ_2\n    ps = rand(N) .< prob\n    R = zeros(N)\n    R[ps] .= d1[ps]\n    R[.!ps] .= d2[.!ps]\n    R\nend","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"Let's use the model to generate some data, this data will constitute our dataset","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"parameters = (1.0, 0.0, 0.2, 2.0, 0.4)\ndata = model(parameters, 5000)","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"let's look at the data","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"using Plots\nhistogram(data)\nsavefig(\"ex1_hist1.svg\");\nnothing; # hide\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"(Image: ex1_hist1)","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"we can now try to infer all parameters using KissABC, first of all we need to define a reasonable prior for our model","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"prior = Factored(\n    Uniform(0, 2), # there is surely a peak between 0 and 2\n    Uniform(-1, 1), #there is a smeared distribution centered around 0\n    Uniform(0, 1), # the peak has surely a width below 1\n    Uniform(0, 4), # the smeared distribution surely has a width less than 4\n    Beta(2, 2), # the number of total events from both distributions look about the same, so we will favor 0.5 just a bit\n);\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"let's look at a sample from the prior, to see that it works","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"rand(prior)","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"now we need a function to compute summary statistics for our data, this is not the optimal choice, but it will work out anyway","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"function S(x)\n    r = (0.1, 0.2, 0.45, 0.55, 0.8, 0.9)\n    quantile(x, r)\nend","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"we will define a function to use the model and summarize it's results","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"summ_model(P, N) = S(model(P, N));\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"now we need a distance function to compare the summary statistics of target data and simulated data","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"summ_data = S(data)\nD(P, N = 5000) = sqrt(mean(abs2, summ_data .- summ_model(P, N)));\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"we can now run ABCDE to get the posterior distribution of our parameters given the dataset data","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"approx_density = ApproxPosterior(prior, D, 0.1)\nres =\n    sample(approx_density, AIS(15), MCMCThreads(), 4000, 4, burnin = 300, progress = false)\n@show res","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"the nominal values of the parameters lie inside the CI.","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = KissABC","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [KissABC]","category":"page"},{"location":"reference/#KissABC.AISChain","page":"Reference","title":"KissABC.AISChain","text":"AISChain(chains::NTuple{N,Vector})\nAISChain(chain::Vector) = AISChain((chain,))\n\nthis type is useful for gathering the results of sample,\n\nExample\n\ngenchain() = [ (rand((1,2,3)), randn()) for i in 1:100] # simple useless generator of chains\nC=AISChain((genchain(),genchain(),genchain(),genchain())) # 4 chains\n\noutput:\n\nObject of type AISChain (total samples 400)\nnumber of samples: 100\nnumber of parameters: 2\nnumber of chains: 4\n┌─────────┬────────────────────┬─────────────────────┬────────────────────┬────────────────────┬───────────────────┐\n│         │               2.5% │               25.0% │              50.0% │              75.0% │             97.5% │\n├─────────┼────────────────────┼─────────────────────┼────────────────────┼────────────────────┼───────────────────┤\n│ Param 1 │                1.0 │                 1.0 │                2.0 │                3.0 │               3.0 │\n│ Param 2 │ -1.830019623768731 │ -0.5840379394249825 │ 0.1015397387884777 │ 0.7357170602574647 │ 1.752198316412034 │\n└─────────┴────────────────────┴─────────────────────┴────────────────────┴────────────────────┴───────────────────┘\n\nindividual samples can be accessed in an 3d-array like fashion:\n\nC[1:90, 1, 1:2] # we are taking from the samples `1:90` of the chains `1:2`, only the parameter `1` \n\noutput:\n\n90×2 Array{Real,2}:\n 2  1\n 2  2\n 2  3\n 1  2\n 1  1\n ⋮\n 2  3\n 1  3\n 3  3\n 2  2\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.ApproxKernelizedPosterior","page":"Reference","title":"KissABC.ApproxKernelizedPosterior","text":"ApproxKernelizedPosterior(\n    prior::Distribution,\n    cost::Function,\n    target_average_cost::Real\n)\n\nthis function will return a type which can be used in the sample function as an ABC density, this type works by assuming Gaussianly distributed errors 𝒩(0,ϵ), ϵ is specified in the variable target_average_cost.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.ApproxPosterior","page":"Reference","title":"KissABC.ApproxPosterior","text":"ApproxPosterior(\n    prior::Distribution,\n    cost::Function,\n    max_cost::Real\n)\n\nthis function will return a type which can be used in the sample function as an ABC density, this type works by assuming uniformly distributed errors in [-ϵ,ϵ], ϵ is specified in the variable max_cost.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.CommonLogDensity","page":"Reference","title":"KissABC.CommonLogDensity","text":"CommonLogDensity(nparameters, sample_init, lπ)\n\nthis function will return a type for performing classical MCMC via the sample function.\n\nnparameters: total number of parameters per sample. sample_init: function which accepts an RNG::AbstractRNG and returns a sample for lπ. lπ: function which accepts a sample, and returns a log-density float value.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.Factored","page":"Reference","title":"KissABC.Factored","text":"Factored{N} <: Distribution{Multivariate, MixedSupport}\n\na Distribution type that can be used to combine multiple UnivariateDistribution's and sample from them. Example: it can be used as prior = Factored(Normal(0,1), Uniform(-1,1))\n\n\n\n\n\n","category":"type"},{"location":"reference/#Base.length-Union{Tuple{Factored{N}}, Tuple{N}} where N","page":"Reference","title":"Base.length","text":"length(p::Factored) = begin\n\nreturns the number of distributions contained in p.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.rand-Union{Tuple{N}, Tuple{Random.AbstractRNG,Factored{N}}} where N","page":"Reference","title":"Base.rand","text":"rand(rng::AbstractRNG, factoreddist::Factored)\n\nfunction to sample one element from a Factored object\n\n\n\n\n\n","category":"method"},{"location":"reference/#Distributions.logpdf-Union{Tuple{N}, Tuple{Factored{N},Any}} where N","page":"Reference","title":"Distributions.logpdf","text":"logpdf(d::Factored, x) = begin\n\nFunction to evaluate the logpdf of a Factored distribution object\n\n\n\n\n\n","category":"method"},{"location":"reference/#Distributions.pdf-Union{Tuple{N}, Tuple{Factored{N},Any}} where N","page":"Reference","title":"Distributions.pdf","text":"pdf(d::Factored, x) = begin\n\nFunction to evaluate the pdf of a Factored distribution object\n\n\n\n\n\n","category":"method"},{"location":"reference/#KissABC.cdf_g_inv-Tuple{Any,Any}","page":"Reference","title":"KissABC.cdf_g_inv","text":"Inverse cdf of g-pdf, see eq. 10 of Foreman-Mackey et al. 2013.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KissABC.sample_g-Tuple{Random.AbstractRNG,Any}","page":"Reference","title":"KissABC.sample_g","text":"Sample from g using inverse transform sampling.  a=2.0 is recommended.\n\n\n\n\n\n","category":"method"},{"location":"reference/#StatsBase.sample","page":"Reference","title":"StatsBase.sample","text":"sample(model, AIS(N), Ns[; optional args])\nsample(model, AIS(N), MCMCThreads(), Ns, Nc[; optional keyword args])\nsample(model, AIS(N), MCMCDistributed(), Ns, Nc[; optional keyword args])\n\nGeneralities\n\nThis function will run an Affine Invariant MCMC sampler, and will return an AISChain object with all the parameter samples, the mandatory parameters are:\n\nmodel: a subtype of AbstractDensity, look at ApproxPosterior, ApproxKernelizedPosterior, CommonLogDensity.\n\nN: number of particles in the ensemble, this particles will be evolved to generate new samples.\n\nNs: total number of samples which must be recorded.\n\nNc: total number of chains to run in parallel if MCMCThreads or MCMCDistributed is enabled.\n\nthe optional arguments available are:\n\nburnin: number of mcmc steps per particle prior to saving any sample.\n\nntransitions: number of mcmc steps per particle between each sample.\n\nprogress: a boolean to disable verbosity\n\nMinimal Example for CommonLogDensity:\n\nusing KissABC\nD = CommonLogDensity(\n    2, #number of parameters\n    rng -> randn(rng, 2), # initial sampling strategy\n    x -> -100 * (x[1] - x[2]^2)^2 - (x[2] - 1)^2, # rosenbrock banana log-density\n)\nres = sample(D, AIS(50), 1000, ntransitions = 100, burnin = 500, progress = false)\nprintln(res)\n\noutput:\n\nnumber of samples: 1000\nnumber of parameters: 2\nnumber of chains: 1\n┌─────────┬───────────────────────┬─────────────────────┬────────────────────┬───────────────────┬────────────────────┐\n│         │                  2.5% │               25.0% │              50.0% │             75.0% │              97.5% │\n├─────────┼───────────────────────┼─────────────────────┼────────────────────┼───────────────────┼────────────────────┤\n│ Param 1 │ -0.025648264131516257 │  0.3219940894353638 │ 0.9721286048546971 │ 2.041743999647929 │ 5.6520319210700825 │\n│ Param 2 │   -0.7226487177325958 │ 0.48611230863899335 │ 0.9604278578610763 │ 1.418519267388806 │  2.385312701114671 │\n└─────────┴───────────────────────┴─────────────────────┴────────────────────┴───────────────────┴────────────────────┘\n\nMinimal Example for ApproxKernelizedPosterior (ApproxPosterior)\n\nusing KissABC, Distributions\nprior = Uniform(-10, 10) # prior distribution for parameter\nsim(μ) = μ + rand((randn() * 0.1, randn())) # simulator function\ncost(x) = abs(sim(x) - 0.0) # cost function to compare simulations to target data, in this case simply '0'\nplan = ApproxPosterior(prior, cost, 0.01) # Approximate model of log-posterior density (ABC)\n#                                           ApproxKernelizedPosterior can be used in the same fashion\nres = sample(plan, AIS(100), 2000, burnin = 10000, progress = false)\nprintln(res)\n\noutput:\n\nObject of type AISChain (total samples 2000)\nnumber of samples: 2000\nnumber of parameters: 1\nnumber of chains: 1\n┌─────────┬─────────────────────┬─────────────────────┬──────────────────────┬─────────────────────┬────────────────────┐\n│         │                2.5% │               25.0% │                50.0% │               75.0% │              97.5% │\n├─────────┼─────────────────────┼─────────────────────┼──────────────────────┼─────────────────────┼────────────────────┤\n│ Param 1 │ -1.8692526364678272 │ -0.1390312701192662 │ 0.023740627510271367 │ 0.20440332127121577 │ 1.1844931848164661 │\n└─────────┴─────────────────────┴─────────────────────┴──────────────────────┴─────────────────────┴────────────────────┘\n\n\n\n\n\n","category":"function"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"EditURL = \"https://github.com/JuliaApproxInference/KissABC.jl/blob/master/docs/literate/index.jl\"","category":"page"},{"location":"#KissABC","page":"Basic Usage","title":"KissABC","text":"","category":"section"},{"location":"#Usage-guide","page":"Basic Usage","title":"Usage guide","text":"","category":"section"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"The ingredients you need to use Approximate Bayesian Computation:","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"A simulation which depends on some parameters, able to generate datasets similar to your target dataset if parameters are tuned\nA prior distribution over such parameters\nA distance function to compare generated dataset to the true dataset","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"We will start with a simple example, we have a dataset generated according to an Normal distribution whose parameters are unknown","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"tdata = randn(1000) .* 0.04 .+ 2;\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"we are ofcourse able to simulate normal random numbers, so this constitutes our simulation","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"sim((μ, σ)) = randn(1000) .* σ .+ μ;\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"The second ingredient is a prior over the parameters μ and σ","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"using Distributions\nusing KissABC\nprior = Factored(Uniform(1, 3), Truncated(Normal(0, 0.1), 0, 100));\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"we have chosen a uniform distribution over the interval [1,3] for μ and a normal distribution truncated over ℝ⁺ for σ.","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"Now all that we need is a distance function to compare the true dataset to the simulated dataset, for this purpose comparing mean and std is optimal","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"function dist(x, y)\n    d1 = mean(x) - mean(y)\n    d2 = std(x) - std(y)\n    hypot(d1, d2 * 50)\nend","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"Now we are all set, we can use mcmc which is Affine Invariant MC algorithm, to simulate the posterior distribution for this model, inferring μ and σ","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"cost(x) = dist(tdata, sim(x))\napprox_density = ApproxPosterior(prior, cost, 0.1)\nres = sample(approx_density, AIS(50), 2000, burnin = 100,ntransitions=10, progress = false)\n@show res","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"the parameters we chose are: a tolerance on distances equal to 0.1, a number of samples equal to 2000, the simulated posterior results are in res. We can now extract the inference results:","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"prsample = [rand(prior) for i = 1:2000] #some samples from the prior for comparison\nμ_pr = getindex.(prsample, 1) # μ samples from the prior\nσ_pr = getindex.(prsample, 2) # σ samples from the prior\nμ_p = res[:, 1, 1] # μ samples from the posterior\nσ_p = res[:, 2, 1]; # σ samples from the posterior\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"and plotting prior and posterior side by side we get:","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"using Plots\na = stephist(\n    μ_pr,\n    xlims = (1, 3),\n    xlabel = \"μ prior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nb = stephist(\n    σ_pr,\n    xlims = (0, 0.3),\n    xlabel = \"σ prior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nap = stephist(\n    μ_p,\n    xlims = (1, 3),\n    xlabel = \"μ posterior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nbp = stephist(\n    σ_p,\n    xlims = (0, 0.3),\n    xlabel = \"σ posterior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nplot(a, ap, b, bp)\nsavefig(\"inference.svg\");\nnothing; # hide\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"(Image: inference_plot)","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"we can see that the algorithm has correctly inferred both parameters, this exact recipe will work for much more complicated models and simulations, with some tuning.","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"This page was generated using Literate.jl.","category":"page"}]
}
