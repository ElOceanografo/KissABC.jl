var documenterSearchIndex = {"docs":
[{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"EditURL = \"https://github.com/JuliaApproxInference/KissABC.jl/blob/master/docs/literate/example_1.jl\"","category":"page"},{"location":"example_1/#A-gaussian-mixture-model","page":"Example: Gaussian Mixture","title":"A gaussian mixture model","text":"","category":"section"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"First of all we define our model,","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"using KissABC\nusing Distributions\n\nfunction model(P, N)\n    μ_1, μ_2, σ_1, σ_2, prob = P\n    d1 = randn(N) .* σ_1 .+ μ_1\n    d2 = randn(N) .* σ_2 .+ μ_2\n    ps = rand(N) .< prob\n    R = zeros(N)\n    R[ps] .= d1[ps]\n    R[.!ps] .= d2[.!ps]\n    R\nend","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"Let's use the model to generate some data, this data will constitute our dataset","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"parameters = (1.0, 0.0, 0.2, 2.0, 0.4)\ndata = model(parameters, 5000)","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"let's look at the data","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"using Plots\nhistogram(data)\nsavefig(\"ex1_hist1.svg\");\nnothing; # hide\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"(Image: ex1_hist1)","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"we can now try to infer all parameters using KissABC, first of all we need to define a reasonable prior for our model","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"prior = Factored(\n    Uniform(0, 2), # there is surely a peak between 0 and 2\n    Uniform(-1, 1), #there is a smeared distribution centered around 0\n    Uniform(0, 1), # the peak has surely a width below 1\n    Uniform(0, 4), # the smeared distribution surely has a width less than 4\n    Beta(2, 2), # the number of total events from both distributions look about the same, so we will favor 0.5 just a bit\n);\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"let's look at a sample from the prior, to see that it works","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"rand(prior)","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"now we need a function to compute summary statistics for our data, this is not the optimal choice, but it will work out anyway","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"function S(x)\n    r = (0.1, 0.2, 0.45, 0.55, 0.8, 0.9)\n    quantile(x, r)\nend","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"we will define a function to use the model and summarize it's results","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"summ_model(P, N) = S(model(P, N));\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"now we need a distance function to compare the summary statistics of target data and simulated data","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"summ_data = S(data)\nD(P, N = 5000) = sqrt(mean(abs2, summ_data .- summ_model(P, N)));\nnothing #hide","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"We can use AIS which is an Affine Invariant MC algorithm via the sample function, to get the posterior distribution of our parameters given the dataset data","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"approx_density = ApproxPosterior(prior, D, 0.1)\nres = sample(\n    approx_density,\n    AIS(50),\n    MCMCThreads(),\n    1000,\n    4,\n    discard_initial = 3000,\n    ntransitions = 10,\n    progress = false,\n)\n@show res","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"the nominal values of the parameters lie inside the CI.","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"","category":"page"},{"location":"example_1/","page":"Example: Gaussian Mixture","title":"Example: Gaussian Mixture","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = KissABC","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [KissABC]","category":"page"},{"location":"reference/#KissABC.ApproxKernelizedPosterior","page":"Reference","title":"KissABC.ApproxKernelizedPosterior","text":"ApproxKernelizedPosterior(\n    prior::Distribution,\n    cost::Function,\n    target_average_cost::Real\n)\n\nthis function will return a type which can be used in the sample function as an ABC density, this type works by assuming Gaussianly distributed errors 𝒩(0,ϵ), ϵ is specified in the variable target_average_cost.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.ApproxPosterior","page":"Reference","title":"KissABC.ApproxPosterior","text":"ApproxPosterior(\n    prior::Distribution,\n    cost::Function,\n    max_cost::Real\n)\n\nthis function will return a type which can be used in the sample function as an ABC density, this type works by assuming uniformly distributed errors in [-ϵ,ϵ], ϵ is specified in the variable max_cost.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.CommonLogDensity","page":"Reference","title":"KissABC.CommonLogDensity","text":"CommonLogDensity(nparameters, sample_init, lπ)\n\nthis function will return a type for performing classical MCMC via the sample function.\n\nnparameters: total number of parameters per sample.\n\nsample_init: function which accepts an RNG::AbstractRNG and returns a sample for lπ.\n\nlπ: function which accepts a sample, and returns a log-density float value.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KissABC.Factored","page":"Reference","title":"KissABC.Factored","text":"Factored{N} <: Distribution{Multivariate, MixedSupport}\n\na Distribution type that can be used to combine multiple UnivariateDistribution's and sample from them. Example: it can be used as prior = Factored(Normal(0,1), Uniform(-1,1))\n\n\n\n\n\n","category":"type"},{"location":"reference/#Base.length-Union{Tuple{Factored{N}}, Tuple{N}} where N","page":"Reference","title":"Base.length","text":"length(p::Factored) = begin\n\nreturns the number of distributions contained in p.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.rand-Union{Tuple{N}, Tuple{Random.AbstractRNG,Factored{N}}} where N","page":"Reference","title":"Base.rand","text":"rand(rng::AbstractRNG, factoreddist::Factored)\n\nfunction to sample one element from a Factored object\n\n\n\n\n\n","category":"method"},{"location":"reference/#Distributions.logpdf-Union{Tuple{N}, Tuple{Factored{N},Any}} where N","page":"Reference","title":"Distributions.logpdf","text":"logpdf(d::Factored, x) = begin\n\nFunction to evaluate the logpdf of a Factored distribution object\n\n\n\n\n\n","category":"method"},{"location":"reference/#Distributions.pdf-Union{Tuple{N}, Tuple{Factored{N},Any}} where N","page":"Reference","title":"Distributions.pdf","text":"pdf(d::Factored, x) = begin\n\nFunction to evaluate the pdf of a Factored distribution object\n\n\n\n\n\n","category":"method"},{"location":"reference/#KissABC.cdf_g_inv-Tuple{Any,Any}","page":"Reference","title":"KissABC.cdf_g_inv","text":"Inverse cdf of g-pdf, see eq. 10 of Foreman-Mackey et al. 2013.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KissABC.sample_g-Tuple{Random.AbstractRNG,Any}","page":"Reference","title":"KissABC.sample_g","text":"Sample from g using inverse transform sampling.  a=2.0 is recommended.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KissABC.smc-Union{Tuple{Tprior}, Tuple{Tprior,Any}} where Tprior<:Distribution","page":"Reference","title":"KissABC.smc","text":"Adaptive SMC from P. Del Moral 2012, with Affine invariant proposal mechanism, faster that AIS for ABC targets.\n\nfunction smc(\n    prior::Distribution,\n    cost::Function;\n    rng::AbstractRNG = Random.GLOBAL_RNG,\n    nparticles::Int = 100,\n    M::Int = 1,\n    alpha = 0.95,\n    mcmc_retrys::Int = 0,\n    mcmc_tol = 0.015,\n    epstol = 0.0,\n    r_epstol = (1 - alpha) / 50,\n    min_r_ess = 0.55,\n    verbose::Bool = false,\n    parallel::Bool = false,\n)\n\nprior: a Distribution object representing the parameters prior.\ncost: a function that given a prior sample returns the cost for said sample (e.g. a distance between simulated data and target data).\nrng: an AbstractRNG object which is used by SMC for inference (it can be useful to make an inference reproducible).\nnparticles: number of total particles to use for inference.\nM: number of cost evaluations per particle, increasing this can reduce the chance of rejecting good particles. \nalpha - used for adaptive tolerance, by solving ESS(n,ϵ(n)) = α ESS(n-1, ϵ(n-1)) for ϵ(n) at step n.\nmcmc_retrys - if set > 0, whenever the fraction of accepted particles drops below the tolerance mcmc_tol the MCMC step is repeated (no more than mcmc_retrys times).\nmcmc_tol - stopping condition for SMC, if the fraction of accepted particles drops below mcmc_tol the algorithm terminates.\nepstol - stopping condition for SMC, if the adaptive cost threshold drops below epstol the algorithm has converged and thus it terminates.\nmin_r_ess - whenever the fractional effective sample size drops below min_r_ess, a systematic resampling step is performed.\nverbose - if set to true, enables verbosity.\nparallel - if set to true, threaded parallelism is enabled, keep in mind that the cost function must be Thread-safe in such case.\n\nExample\n\nusing KissABC\nprior=Factored(Normal(0,5), Normal(0,5))\ncost((x,y)) = 50*(x+randn()*0.01-y^2)^2+(y-1+randn()*0.01)^2\nresults = smc(prior, cost, alpha=0.5, nparticles=5000).P\n\noutput:\n\n2-element Array{Particles{Float64,5000},1}:\n 1.0 ± 0.029\n 0.999 ± 0.012\n\n\n\n\n\n","category":"method"},{"location":"reference/#StatsBase.sample","page":"Reference","title":"StatsBase.sample","text":"sample(model, AIS(N), Ns[; optional args])\nsample(model, AIS(N), MCMCThreads(), Ns, Nc[; optional keyword args])\nsample(model, AIS(N), MCMCDistributed(), Ns, Nc[; optional keyword args])\n\nGeneralities\n\nThis function will run an Affine Invariant MCMC sampler, and will return an Particles object for each parameter, the mandatory parameters are:\n\nmodel: a subtype of AbstractDensity, look at ApproxPosterior, ApproxKernelizedPosterior, CommonLogDensity.\n\nN: number of particles in the ensemble, this particles will be evolved to generate new samples.\n\nNs: total number of samples which must be recorded.\n\nNc: total number of chains to run in parallel if MCMCThreads or MCMCDistributed is enabled.\n\nthe optional arguments available are:\n\ndiscard_initial: number of mcmc particles to discard before saving any sample.\n\nntransitions: number of mcmc steps per particle between each sample.\n\nretry_sampling: number of maximum attempts to resample an initial particle whose cost (or log-density) is ±∞ or NaN.\n\nprogress: a boolean to disable verbosity\n\nMinimal Example for CommonLogDensity:\n\nusing KissABC\nD = CommonLogDensity(\n    2, #number of parameters\n    rng -> randn(rng, 2), # initial sampling strategy\n    x -> -100 * (x[1] - x[2]^2)^2 - (x[2] - 1)^2, # rosenbrock banana log-density\n)\nres = sample(D, AIS(50), 1000, ntransitions = 100, discard_initial = 500, progress = false)\nprintln(res)\n\noutput:\n\nParticles{Float64,1000}[1.43 ± 1.4, 0.99 ± 0.67]\n\nMinimal Example for ApproxKernelizedPosterior (ApproxPosterior)\n\nusing KissABC\nprior = Uniform(-10, 10) # prior distribution for parameter\nsim(μ) = μ + rand((randn() * 0.1, randn())) # simulator function\ncost(x) = abs(sim(x) - 0.0) # cost function to compare simulations to target data, in this case simply '0'\nplan = ApproxPosterior(prior, cost, 0.01) # Approximate model of log-posterior density (ABC)\n#                                           ApproxKernelizedPosterior can be used in the same fashion\nres = sample(plan, AIS(100), 2000, discard_initial = 10000, progress = false)\nprintln(res)\n\noutput:\n\n0.0 ± 0.46\n\n\n\n\n\n","category":"function"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"EditURL = \"https://github.com/JuliaApproxInference/KissABC.jl/blob/master/docs/literate/index.jl\"","category":"page"},{"location":"#KissABC","page":"Basic Usage","title":"KissABC","text":"","category":"section"},{"location":"#Usage-guide","page":"Basic Usage","title":"Usage guide","text":"","category":"section"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"The ingredients you need to use Approximate Bayesian Computation:","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"A simulation which depends on some parameters, able to generate datasets similar to your target dataset if parameters are tuned\nA prior distribution over such parameters\nA distance function to compare generated dataset to the true dataset","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"We will start with a simple example, we have a dataset generated according to an Normal distribution whose parameters are unknown","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"tdata = randn(1000) .* 0.04 .+ 2;\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"we are ofcourse able to simulate normal random numbers, so this constitutes our simulation","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"sim((μ, σ)) = randn(1000) .* σ .+ μ;\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"The second ingredient is a prior over the parameters μ and σ","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"using Distributions\nusing KissABC\nprior = Factored(Uniform(1, 3), Truncated(Normal(0, 0.1), 0, 100));\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"we have chosen a uniform distribution over the interval [1,3] for μ and a normal distribution truncated over ℝ⁺ for σ.","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"Now all that we need is a distance function to compare the true dataset to the simulated dataset, for this purpose comparing mean and std is optimal","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"function dist(x, y)\n    d1 = mean(x) - mean(y)\n    d2 = std(x) - std(y)\n    hypot(d1, d2 * 50)\nend","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"Now we are all set, we can use AIS which is an Affine Invariant MC algorithm via the sample function, to simulate the posterior distribution for this model, inferring μ and σ","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"cost(x) = dist(tdata, sim(x))\napprox_density = ApproxPosterior(prior, cost, 0.1)\nres = sample(\n    approx_density,\n    AIS(50),\n    2000,\n    discard_initial = 1000,\n    ntransitions = 10,\n    progress = false,\n)\n@show res","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"the parameters we chose are: a tolerance on distances equal to 0.1, a number of samples equal to 2000, the simulated posterior results are in res. We can now extract the inference results:","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"prsample = [rand(prior) for i = 1:2000] #some samples from the prior for comparison\nμ_pr = getindex.(prsample, 1) # μ samples from the prior\nσ_pr = getindex.(prsample, 2) # σ samples from the prior\nμ_p = res[1].particles # μ samples from the posterior\nσ_p = res[2].particles; # σ samples from the posterior\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"and plotting prior and posterior side by side we get:","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"using Plots\na = stephist(\n    μ_pr,\n    xlims = (1, 3),\n    xlabel = \"μ prior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nb = stephist(\n    σ_pr,\n    xlims = (0, 0.3),\n    xlabel = \"σ prior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nap = stephist(\n    μ_p,\n    xlims = (1, 3),\n    xlabel = \"μ posterior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nbp = stephist(\n    σ_p,\n    xlims = (0, 0.3),\n    xlabel = \"σ posterior\",\n    leg = false,\n    lw = 2,\n    normalize = true,\n)\nplot(a, ap, b, bp)\nsavefig(\"inference.svg\");\nnothing; # hide\nnothing #hide","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"(Image: inference_plot)","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"we can see that the algorithm has correctly inferred both parameters, this exact recipe will work for much more complicated models and simulations, with some tuning.","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"","category":"page"},{"location":"","page":"Basic Usage","title":"Basic Usage","text":"This page was generated using Literate.jl.","category":"page"}]
}
